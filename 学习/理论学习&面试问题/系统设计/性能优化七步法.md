### 第一步：调用链路优化🔥

+ 一个请求到后端调用的链路一般比较长，涉及多个域，接口有性能问题，首先从架构和领域的视角去分析整个链路，**清晰接口的定位和能力，去除不合理的调用和依赖，做好依赖治理。**
+ 主要有以下几个点需要关注下：
  1. **依赖调用**：去除不合理/无用的调用
  2. **日志打印**：高并发场景，频繁日志打印（比如切面）会导致CPU标高，klog堆积，并发量下降。

### 第二步：SQL优化🔥

1. **慢查询治理**：分离链路中是否存在慢查，对于存在慢查的SQL，**增加查询索引**
2. **热点治理**：分库分表不合理会造成单库、单表热点，需要根据场景选择合适的分库分表策略
3. **查询字段治理**：对于查询语句，需要明确需要查询的字段，**按需查询**；mybatis插件默认全表字段查询，如果表中存在大字段（json、text类型）会导致在高并发场景下堆内存飙升、频繁fullGC
4. **唯一索引治理**：唯一索引对于插入性能有较大影响，团队大量使用唯一索引作为并发控制导致插入性能降低。（ 所有的更新操作都要先判断这个操作是否违反唯一性约束）

### 第三步：串行转并行

资源串行访问，更改为并行处理，减少接口RT，提升TPS/QPS。

<font color='red'>注意：并行执行的任务需要没有依赖关系。</font>

### 第四步： 热点治理

热点问题会存在大量的资源竞争，影响性能，治理这种场景，核心思想就是优化逻辑，**避免出现热点问题**，主要热点场景如下：

1. **热点表**：分库分表的时候需要分析场景，哪个维度的长几个呢流量更多，避免出现热点问题。例如：订阅表根据商家维度分库分表，导致发货的时候出现单表热点，高并发的时候锁竞争变多，QPS上不去，更改订阅表为买家维度分库分表，避免出现表热点问题。

2. **热点数据库**：热点表和普通表需要拆分到不同的库，避免出现热点问题。例如：轨迹表和订阅表在一个库，实际压测发现轨迹表QPS较高，占用了数据库资源，影响了轨迹订阅，后续拆分两个库解决这个问题。

3. **热点服务**：toB和toC的服务的接口请求量和场景都不太一样。

   + B端的服务逻辑复杂，QPS低，迭代频繁，服务等级P2
   + C端的服务逻辑简单，QPS高，迭代少，服务等级P1

   两个服务需要拆分，避免B端和C端相互影响，方便服务动态扩容/缩容

4. **热点缓存**：对于热点缓存，有两种，一种可预期的热点，可以做缓存预热+隔离，另一种是不可预期的，可以做多级缓存（本地缓存+远程缓存）

### 第五步： 缓存资源

1. **kcnf配置缓存**：kconf的配置再美访问的时候会序列化成对象，导致a.JVM内存对象增加；b.消耗CPU资源；需要将序列化的对象做缓存，避免重复生成对象<font color='red'>（kconf的热更新会受到影响）</font>
2. **元数据缓存**：对于快递公司主数据、号段规则等着中读多写少的数据，需要做缓存，减少RPC调用
3. **热点数据缓存**：对于存在数据热点查询的场景，可以缓存热点数据，降低数据库压力。

### 第六步： 线程池调优

1. **gRPC线程数**：默认16，建议设置成300
2. **并行任务线程池：**需要根据压测情况来调整
3. **数据库连接池：**需要设置最小的连接数（一般是5），最大连接数默认是10

+ 线程池的核心线程数设置技巧：
  + IO密集型的任务：2N（N是核数）=16（经验值），实际上对于IO密集型的场景，CPU不是瓶颈，尽量设置大点，推荐设置为100（电子面单、轨迹设置的数量）。实际场景需要根据压测情况不断调整。
  + CPU密集型任务：N+1=9（经验值）

### 第七步： JVM调优

根据不同的场景，可以对机器的JVM进行调优，常见调优方法如下

1. 调大堆的大小：堆太小会导致频繁FullGC，根据业务场景，设置合适的堆大小，能对性能有所优化
2. 更改垃圾收集器：G1收集器相对CMS在垃圾收集算法（标记整理）和STW时间上都有着明显的提升，可以更换垃圾收集器来提升性能。



## **案例一： 正向发货校验性能优化**

<img src="https://raw.githubusercontent.com/tuysss/cloudimg/main/Typora-Notes-images/2022/12/09/3a1a5a2d0c301a231a174ddaf2cf1ee6-20221209194525-bcb715.png" alt="image-20221209194523057" style="zoom:50%;" />

问题：正向发货订阅项目的接口RT较长（1.2s），不满足上游的SLA要求（P99在100ms内）

方法：①**调用链路优化、④热点治理**

解法：

1、**评估SLA合理性**：供应链创建订阅单的接口其实本身定位是申请履约（applyFulfil），受理履约请求，RT应该在200ms左右

2、**依赖治理：**调用链路中、交付和供应链重复查询了订单和商品信息，导致整体接口RT变长，供应链去除交易和商品的调用，改由交付下发，理由RT减少500ms

3、**热点问题治理**：

- 单表热点：

- - 现状：商家发货存在单商家1k+QPS的案例，订阅表按照商家维度分库分表，导致单表存在热点问题，影响整体性能；
  - 方案：订阅表改为使用买家维度分库分表，保证数据的离散性

- 数据库热点

- - 现状：轨迹表和订阅表在同一个库，轨迹回流QPS较高，导致数据库热点，影响轨迹订阅
  - 方案：轨迹表和订阅表拆分为两个库，轨迹表单独隔离，保证轨迹订阅不受影响

*结果：P99为72ms*



## **案例二：订单列表外漏轨迹性能优化**

<img src="https://raw.githubusercontent.com/tuysss/cloudimg/main/Typora-Notes-images/2022/12/09/58685aa8158ad53eebebb0a6eec58e51-20221209194702-1ff410.png" alt="image-20221209194701166" style="zoom: 33%;" />

问题：批量查询轨迹的接口需要支持单机1k的QPS（批量10条数据）

方法： **②SQL优化 ③串行转并行 ⑤缓存资源 ⑥线程池调优**

解法：

**1、SQL优化**：订阅表有一个logistics_detail的大字段，储存轨迹详情，查询的时候去除大字段的查询，降低对象大小

**2、串行转并行**：20条轨迹查询串行需要400ms，分成10个线程处理，每个线程查询2条轨迹，降低RT

**3、缓存资源**：快递公司基础数据配置在kconf，每次查询都会序列化为对象，导致JVM飙升，需要缓存实例对象，减少对象生成

**4、线程池调优：**gRpc线程数和并行任务线程数会相互影响，需要不断压测调整参数



结果：5条轨迹单机1.1k QPS，10条轨迹 600 QPS，整体RT在20ms



## **案例三：电子面单取号性能优化**

<img src="https://raw.githubusercontent.com/tuysss/cloudimg/main/Typora-Notes-images/2022/12/09/bcd7c63b92db882e906077e5d55f03b4-20221209194855-2db8b1.png" alt="image-20221209194853072" style="zoom: 50%;" />

问题：批量取号需要支持集群1kQPS

方法：**③串行转并行 ④热点治理 ⑤缓存资源 ⑥线程池调优**

解法**：**

**1、串行转并行**：10条取号请求串行需要1s，分成10个线程处理，每个线程查询1个取号请求，降低RT

**2、热点治理**：单个商家的电子账户扣减存在热点，合并多个相同请求为一个请求，减少账户扣减次数

**3、缓存资源：**号段规则、快递公司主数据写少读多，可以做缓存，减少调用



结果：取号数量为1时，单机QPS达到600；取号数量10条，单机100QPS





